# AWS IAM Assistant Chatbot

<div align="center">
  <img src="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/svgs/solid/user-lock.svg" width="100">
  <img src="https://cdn.jsdelivr.net/npm/@mdi/svg@7/svg/cloud-lock-outline.svg" width="100">
  
  <h1>AWS IAM Assistant</h1>
</div>

A local AI-powered chatbot that helps with AWS IAM policy generation, troubleshooting, and best practices explanations using Ollama and FastAPI.

## Features

- ğŸ› ï¸ Generate valid IAM policies based on natural language requests
- ğŸ” Explain IAM concepts in simple terms
- ğŸš¨ Troubleshoot common IAM errors
- ğŸ’» 100% local operation - no AWS credentials required
- ğŸš€ FastAPI backend with simple web interface

## Prerequisites

- macOS or Linux (Windows via WSL2)
- Python 3.9+
- Ollama (will be installed automatically)
- [Optional] NVIDIA GPU for better performance

## Installation

```bash
# Clone the repository
git clone https://github.com/sanjibbehera/hackathon-newideas-project.git
cd hackathon-newideas-project

# Install Python dependencies
pip install -r requirements.txt

# Run setup script (installs Ollama if missing)
./scripts/setup_ollama.sh

# Start the application
uvicorn app.main:app --reload
```

## Usage
- Access the web interface at http://localhost:8000
- Ask questions like:
  - "Create a read-only policy for S3 bucket 'my-data'"
  - "Explain IAM roles vs users"
  - "Why am I getting AccessDenied for EC2?"

## API Endpoints
```bash
POST /api/chat
Content-Type: application/json

{
  "message": "How do I restrict S3 access by IP?"
}
```

## Project Structure
```bash
hackathon-newideas-project/
â”œâ”€â”€ app/                        # FastAPI application
â”‚   â”œâ”€â”€ main.py                 # Main application
â”‚   â”œâ”€â”€ models/                 # Data models
â”‚   â””â”€â”€ routes/                 # API routes
â”œâ”€â”€ iam_knowledge/              # Training data
â”‚   â”œâ”€â”€ policies/               # Policy examples
â”‚   â””â”€â”€ common_questions.json   # Common questions
â”œâ”€â”€ ollama/                     # Ollama configuration
â”‚   â”œâ”€â”€ Modelfile               # Model definition
â”‚   â””â”€â”€ setup_ollama.sh         # Ollama setup
â”œâ”€â”€ static/                     # Frontend assets
â”‚   â”œâ”€â”€ css/                    # CSS Files
â”‚   â”œâ”€â”€ images/                 # Images Files
â”‚   â””â”€â”€ js                      # Javascript Files
â”œâ”€â”€ templates/                  # HTML templates
â”œâ”€â”€ scripts/                    # Utility scripts
â”‚   â””â”€â”€ train_model.sh          # Model training
â”œâ”€â”€ tests/                      # Tests (Pytest)
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ pytest.ini                  # Pytest config
â””â”€â”€ README.md                   # Project docs
```

## ğŸ“¸ Screenshots

### Chatbot Frontend
![Chatbot Interface](./images/chatbot_frontend_screenshot.png)

## Build the LLM model

```bash
ollama pull mistral
ollama create aws-expert -f ./data/aws_expert.Modelfile
```

## Contributing
1. Fork the repository
2. Create a new branch (git checkout -b feature/your-feature)
3. Commit your changes
4. Push to the branch
5. Open a pull request